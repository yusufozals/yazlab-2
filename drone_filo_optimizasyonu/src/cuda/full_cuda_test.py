# full_cuda_test.py
import time
import torch
import numpy as np


def full_cuda_test():
    """KapsamlÄ± CUDA testi"""
    print("ğŸš€ KAPSAMLI CUDA TESTÄ°")
    print("=" * 60)

    # 1. Sistem Bilgileri
    print("ğŸ“Š Sistem Bilgileri:")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")

    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"GPU {i}: {props.name}")
        print(f"  Memory: {props.total_memory / 1e9:.1f} GB")
        print(f"  Compute: {props.major}.{props.minor}")

    # 2. CuPy kontrolÃ¼
    try:
        import cupy as cp
        print(f"\nâœ… CuPy version: {cp.__version__}")
        print(f"CuPy CUDA version: {cp.cuda.runtime.runtimeGetVersion()}")
    except ImportError:
        print("\nâŒ CuPy yÃ¼klÃ¼ deÄŸil")

    # 3. Numba CUDA kontrolÃ¼
    try:
        from numba import cuda
        print(f"\nâœ… Numba CUDA available")
        print(f"Numba GPU count: {len(cuda.gpus)}")
        for i, gpu in enumerate(cuda.gpus):
            print(f"  GPU {i}: {gpu.name}")
    except Exception as e:
        print(f"\nâŒ Numba CUDA hatasÄ±: {e}")

    # 4. Performans Testleri
    print(f"\nâš¡ PERFORMANS TESTLERÄ°")
    print("-" * 40)

    # Test boyutlarÄ±
    sizes = [1000, 2000, 5000]

    for size in sizes:
        print(f"\nğŸ“ Test boyutu: {size}x{size}")

        # CPU test
        a_cpu = torch.randn(size, size)
        b_cpu = torch.randn(size, size)

        start = time.time()
        c_cpu = torch.matmul(a_cpu, b_cpu)
        cpu_time = time.time() - start
        print(f"  CPU: {cpu_time:.3f}s")

        # GPU test
        device = torch.device('cuda')
        a_gpu = torch.randn(size, size, device=device)
        b_gpu = torch.randn(size, size, device=device)

        # Warm-up
        torch.cuda.synchronize()
        _ = torch.matmul(a_gpu, b_gpu)
        torch.cuda.synchronize()

        # Actual test
        start = time.time()
        c_gpu = torch.matmul(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start

        speedup = cpu_time / gpu_time if gpu_time > 0 else 0
        print(f"  GPU: {gpu_time:.3f}s")
        print(f"  ğŸš€ HÄ±zlanma: {speedup:.1f}x")

        # DoÄŸruluk kontrolÃ¼
        c_gpu_cpu = c_gpu.cpu()
        max_diff = torch.max(torch.abs(c_cpu - c_gpu_cpu)).item()
        print(f"  âœ“ DoÄŸruluk: {max_diff:.2e}")

    # 5. Memory Test
    print(f"\nğŸ’¾ MEMORY TESTÄ°")
    print("-" * 20)

    device = torch.device('cuda')

    # Mevcut memory
    allocated = torch.cuda.memory_allocated(device) / 1e6
    cached = torch.cuda.memory_reserved(device) / 1e6
    print(f"Allocated: {allocated:.1f} MB")
    print(f"Cached: {cached:.1f} MB")

    # BÃ¼yÃ¼k tensor oluÅŸtur
    try:
        big_tensor = torch.randn(10000, 10000, device=device)
        allocated_after = torch.cuda.memory_allocated(device) / 1e6
        print(f"Big tensor allocated: {allocated_after - allocated:.1f} MB")
        del big_tensor
        torch.cuda.empty_cache()
        print("âœ… Memory test baÅŸarÄ±lÄ±")
    except RuntimeError as e:
        print(f"âŒ Memory test hatasÄ±: {e}")

    # 6. Genel DeÄŸerlendirme
    print(f"\nğŸ¯ SONUÃ‡")
    print("-" * 15)

    if torch.cuda.is_available():
        print("âœ… CUDA tam olarak Ã§alÄ±ÅŸÄ±yor!")
        print("âœ… GPU hesaplamalar iÃ§in hazÄ±r")
        print("ğŸš€ Drone projesi CUDA ile hÄ±zlandÄ±rÄ±labilir")

        # Ã–neriler
        print(f"\nğŸ’¡ Ã–NERÄ°LER:")
        print("1. Genetik algoritma popÃ¼lasyon boyutunu artÄ±rÄ±n (100 â†’ 200)")
        print("2. Mesafe hesaplamalarÄ±nÄ± GPU'ya taÅŸÄ±yÄ±n")
        print("3. Fitness hesaplamalarÄ±nÄ± batch olarak yapÄ±n")

        return True
    else:
        print("âŒ CUDA problemi var")
        return False


if __name__ == "__main__":
    success = full_cuda_test()

    if success:
        print(f"\nğŸ‰ CUDA hazÄ±r! Projenizde aÅŸaÄŸÄ±daki kodu kullanabilirsiniz:")
        print("""
# CUDA device ayarlama
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# Tensor'larÄ± GPU'ya taÅŸÄ±ma
tensor_gpu = torch.tensor(data, device=device)
        """)